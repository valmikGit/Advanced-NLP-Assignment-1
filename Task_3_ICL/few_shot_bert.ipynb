{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc05f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "few_shot_cls_head_per_query.py\n",
    "\n",
    "Few-shot classification with frozen BERT encoder + randomly initialized classification head per query.\n",
    "\n",
    "Pipeline:\n",
    "1. Randomly choose a row from the CSV -> this is the query we want to classify.\n",
    "2. Randomly select n other distinct rows as \"shots\" (support set).\n",
    "3. Randomly initialize the classification head weights.\n",
    "4. Train the classification head on the n shots (with frozen BERT).\n",
    "5. Infer on the target query using the temporarily learned head.\n",
    "6. Repeat for num_queries times, save predictions to CSV and print accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "# -------------------------\n",
    "# Argument parsing\n",
    "# -------------------------\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Few-shot classification with reinitialized head per query\")\n",
    "    p.add_argument(\"--csv\", type=str, default=\"mt_bench_training.csv\", help=\"Input CSV file path\")\n",
    "    p.add_argument(\"--output_csv\", type=str, default=\"bert_few_shot_cls_predictions.csv\", help=\"Output CSV file path\")\n",
    "    p.add_argument(\"--model_name\", type=str, default=\"bert-base-uncased\", help=\"Pretrained BERT encoder\")\n",
    "    p.add_argument(\"--shots\", type=int, default=1, help=\"Number of support examples per query\")\n",
    "    p.add_argument(\"--num_queries\", type=int, default=100, help=\"Number of queries to evaluate\")\n",
    "    p.add_argument(\"--epochs\", type=int, default=10, help=\"Training epochs for head on support set\")\n",
    "    p.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate for head training\")\n",
    "    p.add_argument(\"--max_length\", type=int, default=256, help=\"Max token length\")\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    return p.parse_args()\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "def build_text_from_row(row):\n",
    "    \"\"\"Format row into natural language text.\"\"\"\n",
    "    turn = int(row[\"turn\"])\n",
    "    q1 = str(row.get(\"turn_1_query\", \"\")).strip()\n",
    "    if turn == 1:\n",
    "        return f\"Turn 1 query: {q1}\"\n",
    "    elif turn == 2:\n",
    "        ans = str(row.get(\"turn_1_answer\", \"\")).strip()\n",
    "        q2 = str(row.get(\"turn_2_query\", \"\")).strip()\n",
    "        return f\"Turn 1 query: {q1}, Turn 1 answer: {ans}, Turn 2 query: {q2}\"\n",
    "    else:\n",
    "        return f\"Turn 1 query: {q1}\"\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"Simple linear classification head on top of CLS embedding.\"\"\"\n",
    "    def __init__(self, hidden_size, num_labels):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def encode_texts(texts, tokenizer, model, max_length, device):\n",
    "    \"\"\"Encode texts with frozen BERT, return CLS embeddings.\"\"\"\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**enc)\n",
    "        cls_embeds = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "    return cls_embeds\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load dataset\n",
    "    if not os.path.exists(args.csv):\n",
    "        raise FileNotFoundError(f\"CSV not found: {args.csv}\")\n",
    "    df = pd.read_csv(args.csv)\n",
    "    unique_labels = sorted(df[\"winner\"].unique().tolist())\n",
    "    label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "    id2label = {i: label for i, label in enumerate(unique_labels)}\n",
    "    print(f\"Unique labels: {unique_labels}\")\n",
    "\n",
    "    # Load frozen BERT encoder\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "    bert = AutoModel.from_pretrained(args.model_name).to(device)\n",
    "    bert.eval()\n",
    "    for param in bert.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    results = []\n",
    "    correct = 0\n",
    "\n",
    "    for i in trange(args.num_queries, desc=\"Queries\"):\n",
    "        # 1. Pick target row\n",
    "        target_idx = random.randint(0, len(df) - 1)\n",
    "        target_row = df.iloc[target_idx]\n",
    "\n",
    "        # 2. Select support set\n",
    "        support_indices = random.sample(\n",
    "            [j for j in range(len(df)) if j != target_idx],\n",
    "            k=min(args.shots, len(df)-1)\n",
    "        )\n",
    "        support_rows = [df.iloc[j] for j in support_indices]\n",
    "\n",
    "        # 3. Prepare support texts and labels\n",
    "        support_texts = [build_text_from_row(r) for r in support_rows]\n",
    "        support_labels = torch.tensor([label2id[r[\"winner\"]] for r in support_rows], dtype=torch.long).to(device)\n",
    "\n",
    "        # 4. Encode support set\n",
    "        support_embeds = encode_texts(support_texts, tokenizer, bert, args.max_length, device)\n",
    "\n",
    "        # 5. Randomly initialize classification head\n",
    "        head = ClassificationHead(bert.config.hidden_size, len(unique_labels)).to(device)\n",
    "\n",
    "        # 6. Train classification head on support set\n",
    "        optimizer = optim.AdamW(head.parameters(), lr=args.lr)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        head.train()\n",
    "        for epoch in range(args.epochs):\n",
    "            optimizer.zero_grad()\n",
    "            logits = head(support_embeds)\n",
    "            loss = loss_fn(logits, support_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 7. Encode target query\n",
    "        target_text = build_text_from_row(target_row)\n",
    "        target_embed = encode_texts([target_text], tokenizer, bert, args.max_length, device)\n",
    "\n",
    "        # 8. Infer with trained head\n",
    "        head.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = head(target_embed)\n",
    "            probs = torch.softmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "            pred_id = int(np.argmax(probs))\n",
    "            pred_label = id2label[pred_id]\n",
    "\n",
    "        # 9. Update accuracy counter\n",
    "        if pred_label == target_row[\"winner\"]:\n",
    "            correct += 1\n",
    "\n",
    "        # 10. Save result\n",
    "        results.append({\n",
    "            \"query_id\": target_row[\"question_id\"],\n",
    "            \"turn\": target_row[\"turn\"],\n",
    "            \"true_winner\": target_row[\"winner\"],\n",
    "            \"predicted_winner\": pred_label,\n",
    "            \"support_set\": [r[\"question_id\"] for r in support_rows],\n",
    "            \"probs\": {id2label[j]: float(probs[j]) for j in range(len(probs))}\n",
    "        })\n",
    "\n",
    "        print(f\"[{i+1}/{args.num_queries}] True: {target_row['winner']} | Pred: {pred_label}\")\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = correct / args.num_queries\n",
    "    print(f\"\\nFinal Accuracy over {args.num_queries} queries: {accuracy:.4f}\")\n",
    "\n",
    "    # Save predictions\n",
    "    out_df = pd.DataFrame(results)\n",
    "    out_df.to_csv(args.output_csv, index=False)\n",
    "    print(f\"Saved predictions to {args.output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignmentenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
