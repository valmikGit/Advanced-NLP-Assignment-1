{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20923c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# ----------------------\n",
    "# 1. Load model + tokenizer\n",
    "# ----------------------\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ----------------------\n",
    "# 2. Define candidate labels\n",
    "# ----------------------\n",
    "candidate_labels = [\"gpt-3.5-turbo\", \"vicuna-13b-v1.2\", \"alpaca-13b\", \"claude-v1\"]\n",
    "\n",
    "# Tokenize each label (some may split into multiple subtokens)\n",
    "label_token_ids = []\n",
    "for lbl in candidate_labels:\n",
    "    tokens = tokenizer.tokenize(lbl.lower())  # normalize case\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    label_token_ids.append(ids)\n",
    "    print(f\"Label '{lbl}' -> tokens {tokens} -> ids {ids}\")\n",
    "\n",
    "# ----------------------\n",
    "# 3. Build a few-shot prompt with [MASK]\n",
    "# ----------------------\n",
    "prompt = \"\"\"\n",
    "You are given a dataset of user prompts, model responses, and follow-up tasks.\n",
    "Decide which model is the winner based on quality of response.\n",
    "\n",
    "Query: Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.\n",
    "Winner: GPT-3.5-turbo\n",
    "Query: Draft a professional email seeking your supervisor's feedback on the 'Quarterly Financial Report' you prepared. Ask specifically about the data analysis, presentation style, and the clarity of conclusions drawn. Keep the email short and to the point.\n",
    "Winner: GPT-3.5-turbo\n",
    "Query: Write a persuasive email to convince your introverted friend, who dislikes public speaking, to volunteer as a guest speaker at a local event. Use compelling arguments and address potential objections. Please be concise.\n",
    "Winner: [MASK]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "mask_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "# ----------------------\n",
    "# 4. Forward pass\n",
    "# ----------------------\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "mask_logits = logits[0, mask_index, :]  # shape: [1, vocab_size]\n",
    "\n",
    "# ----------------------\n",
    "# 5. Compute restricted probabilities\n",
    "# ----------------------\n",
    "def score_label(token_ids):\n",
    "    \"\"\"Compute mean log-prob across subtokens for multi-token labels.\"\"\"\n",
    "    log_probs = []\n",
    "    for tid in token_ids:\n",
    "        prob = torch.log_softmax(mask_logits, dim=-1)[0, tid]\n",
    "        log_probs.append(prob)\n",
    "    return torch.stack(log_probs).mean().item()\n",
    "\n",
    "scores = [score_label(ids) for ids in label_token_ids]\n",
    "\n",
    "# Normalize into probabilities\n",
    "probs = torch.softmax(torch.tensor(scores), dim=-1)\n",
    "\n",
    "# ----------------------\n",
    "# 6. Print results\n",
    "# ----------------------\n",
    "print(\"\\n--- Predictions ---\")\n",
    "for lbl, p in zip(candidate_labels, probs.tolist()):\n",
    "    print(f\"{lbl}: {p:.4f}\")\n",
    "\n",
    "predicted = candidate_labels[torch.argmax(probs)]\n",
    "print(f\"\\nPredicted label: {predicted}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignmentenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
