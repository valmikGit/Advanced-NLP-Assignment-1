{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74cc9de9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Any\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset, Dataset, ClassLabel\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     AutoTokenizer,\n\u001b[32m     24\u001b[39m     AutoModelForSequenceClassification,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     DataCollatorWithPadding,\n\u001b[32m     28\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model, TaskType\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "fine_tune_bert_lora_classification.py\n",
    "\n",
    "Fine-tune a BERT model (e.g., bert-base-uncased) for sequence classification\n",
    "using LoRA (PEFT). The script uses a three-way split: train, validation, and test.\n",
    "\n",
    "This script is adapted for an encoder-only architecture. The task is framed as a\n",
    "multi-class classification problem where the model predicts the ID of the best\n",
    "model for a given query.\n",
    "\n",
    "The input CSV contains conversational turns, which are formatted into a single\n",
    "input sequence for BERT.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, ClassLabel\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------\n",
    "# Argument parsing\n",
    "# -------------------------\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Fine-tune a BERT model with LoRA for sequence classification.\")\n",
    "    p.add_argument(\"--csv\", type=str, default=\"mt_bench_training.csv\", help=\"Input CSV file path\")\n",
    "    p.add_argument(\"--output_dir\", type=str, default=\"./bert_peft_adapter\", help=\"Where to save the LoRA adapter\")\n",
    "    p.add_argument(\"--model_name\", type=str, default=\"bert-base-uncased\", help=\"Base encoder-only model\")\n",
    "    p.add_argument(\"--batch_size\", type=int, default=8)\n",
    "    p.add_argument(\"--epochs\", type=int, default=3)\n",
    "    p.add_argument(\"--lr\", type=float, default=5e-5)\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    p.add_argument(\"--max_input_length\", type=int, default=512) # BERT's max length\n",
    "    p.add_argument(\"--lora_r\", type=int, default=8)\n",
    "    p.add_argument(\"--lora_alpha\", type=int, default=16)\n",
    "    p.add_argument(\"--lora_dropout\", type=float, default=0.1)\n",
    "    p.add_argument(\"--save_total_limit\", type=int, default=2)\n",
    "    p.add_argument(\"--eval_steps\", type=int, default=200)\n",
    "    p.add_argument(\"--logging_steps\", type=int, default=50)\n",
    "    p.add_argument(\"--seed_data_split\", type=int, default=42)\n",
    "    p.add_argument(\"--test_size\", type=float, default=0.1, help=\"Fraction for the final test set.\")\n",
    "    p.add_argument(\"--validation_size\", type=float, default=0.1, help=\"Fraction of non-test data for validation.\")\n",
    "    return p.parse_args()\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "def build_input_text_from_row(row: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Build a single input text string for BERT based on the 'turn' value.\n",
    "    This text will be fed to the encoder.\n",
    "    \"\"\"\n",
    "    turn = int(row[\"turn\"])\n",
    "    q1 = str(row.get(\"turn_1_query\", \"\")).strip()\n",
    "    if turn == 1:\n",
    "        # For BERT, we provide the full context as a single string\n",
    "        text = f\"Query: {q1}\"\n",
    "    elif turn == 2:\n",
    "        ans = str(row.get(\"turn_1_answer\", \"\")).strip()\n",
    "        q2 = str(row.get(\"turn_2_query\", \"\")).strip()\n",
    "        # Concatenate the conversation history\n",
    "        text = f\"Query: {q1} [SEP] Answer: {ans} [SEP] Follow-up Query: {q2}\"\n",
    "    else:\n",
    "        text = f\"Query: {q1}\"\n",
    "    return text\n",
    "\n",
    "def preprocess_dataset(records: list, tokenizer, args):\n",
    "    \"\"\"\n",
    "    Tokenize the input text. The label is already an integer and doesn't need tokenization.\n",
    "    Accepts a list of dicts.\n",
    "    \"\"\"\n",
    "    # Build the single text input for BERT from the row data\n",
    "    text_inputs = [build_input_text_from_row(ex) for ex in records]\n",
    "    \n",
    "    # Tokenize the constructed text\n",
    "    model_inputs = tokenizer(\n",
    "        text_inputs,\n",
    "        max_length=args.max_input_length,\n",
    "        truncation=True,\n",
    "        padding=False, # Collator will handle padding\n",
    "    )\n",
    "    return Dataset.from_dict(model_inputs)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Compute Metrics\n",
    "# -------------------------\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"\n",
    "    Compute accuracy from model predictions (logits) and true labels.\n",
    "    \"\"\"\n",
    "    preds, labels = eval_preds\n",
    "    # The predictions are logits; we need to take the argmax to get the predicted class ID\n",
    "    pred_ids = np.argmax(preds, axis=1)\n",
    "    \n",
    "    acc = accuracy_metric.compute(predictions=pred_ids, references=labels)\n",
    "    return {\"accuracy\": acc[\"accuracy\"]}\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    # --- ADDED: Explicitly check for CUDA and set device ---\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"--- Using device: {device.upper()} ---\")\n",
    "    # The Trainer will automatically use the GPU if it's available.\n",
    "    # This check is for user information.\n",
    "\n",
    "    # Load CSV as a pandas DataFrame to easily create the label mapping\n",
    "    if not os.path.exists(args.csv):\n",
    "        raise FileNotFoundError(f\"CSV file not found: {args.csv}\")\n",
    "    \n",
    "    df = pd.read_csv(args.csv)\n",
    "    # NOTE: Assuming 'winner' column has no ties, as per the user request.\n",
    "    \n",
    "    # --- Create Label Mappings ---\n",
    "    unique_winners = sorted(df[\"winner\"].unique().tolist()) # Sort for consistency\n",
    "    label2id = {label: i for i, label in enumerate(unique_winners)}\n",
    "    id2label = {i: label for i, label in enumerate(unique_winners)}\n",
    "    num_labels = len(unique_winners)\n",
    "    print(f\"Found {num_labels} unique labels: {unique_winners}\")\n",
    "    \n",
    "    # Add the integer 'label' column to the DataFrame\n",
    "    df['label'] = df['winner'].map(label2id)\n",
    "    \n",
    "    # Load the DataFrame as a Hugging Face Dataset\n",
    "    raw_all = Dataset.from_pandas(df)\n",
    "\n",
    "    # --- Create Train, Validation, and Test Splits ---\n",
    "    train_val_split = raw_all.train_test_split(test_size=args.test_size, seed=args.seed_data_split, stratify_by_column=\"label\")\n",
    "    train_val_ds = train_val_split[\"train\"]\n",
    "    test_ds = train_val_split[\"test\"]\n",
    "\n",
    "    train_split = train_val_ds.train_test_split(test_size=args.validation_size, seed=args.seed_data_split, stratify_by_column=\"label\")\n",
    "    train_ds = train_split[\"train\"]\n",
    "    val_ds = train_split[\"test\"]\n",
    "\n",
    "    print(f\"Dataset splits created: train={len(train_ds)}, validation={len(val_ds)}, test={len(test_ds)}\")\n",
    "\n",
    "    # Load tokenizer and model for Sequence Classification\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        args.model_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    \n",
    "    # LoRA (PEFT) configuration for Sequence Classification\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=args.lora_r,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        target_modules=[\"query\", \"value\"],\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    print(\"Wrapped model with LoRA for Sequence Classification. Trainable parameters:\")\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # Tokenize datasets\n",
    "    tokenized_train = preprocess_dataset(train_ds.to_dict('records'), tokenizer, args)\n",
    "    tokenized_val = preprocess_dataset(val_ds.to_dict('records'), tokenizer, args)\n",
    "    tokenized_test = preprocess_dataset(test_ds.to_dict('records'), tokenizer, args)\n",
    "\n",
    "    # Add the labels back to the tokenized datasets\n",
    "    tokenized_train = tokenized_train.add_column(\"labels\", train_ds[\"label\"])\n",
    "    tokenized_val = tokenized_val.add_column(\"labels\", val_ds[\"label\"])\n",
    "    tokenized_test = tokenized_test.add_column(\"labels\", test_ds[\"label\"])\n",
    "\n",
    "    # Data collator for padding\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Standard TrainingArguments (not Seq2Seq)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        num_train_epochs=args.epochs,\n",
    "        learning_rate=args.lr,\n",
    "        save_total_limit=args.save_total_limit,\n",
    "        fp16=torch.cuda.is_available(), # This line enables CUDA usage\n",
    "        logging_steps=args.logging_steps,\n",
    "        eval_steps=args.eval_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=args.eval_steps,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        seed=args.seed,\n",
    "    )\n",
    "\n",
    "    # Standard Trainer (not Seq2SeqTrainer)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"--- Starting Training ---\")\n",
    "    trainer.train()\n",
    "    print(\"--- Training Finished ---\")\n",
    "\n",
    "    # Final Evaluation on the held-out Test Set\n",
    "    print(\"\\n--- Evaluating on the held-out Test Set ---\")\n",
    "    test_results = trainer.evaluate(eval_dataset=tokenized_test)\n",
    "    print(\"Test Set Metrics:\")\n",
    "    print(test_results)\n",
    "\n",
    "    # Save the final LoRA adapter and tokenizer\n",
    "    print(\"\\nSaving final PEFT adapter to:\", args.output_dir)\n",
    "    model.save_pretrained(args.output_dir)\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
