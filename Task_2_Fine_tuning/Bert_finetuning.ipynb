{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvAcADaEFABn9xW+sP8obt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Model Fine-Tuning\n",
        "BERT base uncased"
      ],
      "metadata": {
        "id": "UuE1qEHKeWY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets peft evaluate accelerate bitsandbytes pandas -q"
      ],
      "metadata": {
        "id": "5JeniNOevH24"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "IlWSP8TAwaNM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset, ClassLabel, Features, Value\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# -------------------------\n",
        "# Configuration\n",
        "# -------------------------\n",
        "class SimpleArgs:\n",
        "    csv = \"mt_bench_training.csv\"\n",
        "    output_dir = \"./model_adapter\"\n",
        "    model_name = \"bert-base-uncased\"  # <-- switched to encoder-only\n",
        "    batch_size = 8\n",
        "    epochs = 3\n",
        "    lr = 5e-5\n",
        "    seed = 42\n",
        "    max_input_length = 512\n",
        "    lora_r = 16\n",
        "    lora_alpha = 32\n",
        "    lora_dropout = 0.05\n",
        "    save_total_limit = 2\n",
        "    eval_steps = 10\n",
        "    logging_steps = 10\n",
        "    seed_data_split = 42\n",
        "    test_size = 0.1\n",
        "    validation_size = 0.1\n",
        "\n",
        "args = SimpleArgs()\n",
        "\n",
        "# -------------------------\n",
        "# Utilities\n",
        "# -------------------------\n",
        "def build_input_texts_from_columns(examples: Dict[str, List], tokenizer) -> List[str]:\n",
        "    text_inputs = []\n",
        "    sep_token = tokenizer.sep_token if tokenizer.sep_token is not None else \" \"\n",
        "\n",
        "    for i in range(len(examples[\"turn\"])):\n",
        "        turn = int(examples[\"turn\"][i])\n",
        "        q1 = str(examples.get(\"turn_1_query\", [\"\"])[i]).strip()\n",
        "\n",
        "        if turn == 1:\n",
        "            text = f\"Query: {q1}\"\n",
        "        elif turn == 2:\n",
        "            ans = str(examples.get(\"turn_1_answer\", [\"\"])[i]).strip()\n",
        "            q2 = str(examples.get(\"turn_2_query\", [\"\"])[i]).strip()\n",
        "            text = f\"Query: {q1}{sep_token}Answer: {ans}{sep_token}Follow-up Query: {q2}\"\n",
        "        else:\n",
        "            text = f\"Query: {q1}\"\n",
        "        text_inputs.append(text)\n",
        "    return text_inputs\n",
        "\n",
        "\n",
        "def preprocess_function(examples, tokenizer, args):\n",
        "    text_inputs = build_input_texts_from_columns(examples, tokenizer)\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        text_inputs,\n",
        "        max_length=args.max_input_length,\n",
        "        truncation=True,\n",
        "        padding=False,\n",
        "    )\n",
        "    model_inputs[\"labels\"] = examples[\"label\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Compute Metrics\n",
        "# -------------------------\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        logits = preds[0]\n",
        "    else:\n",
        "        logits = preds\n",
        "\n",
        "    pred_ids = np.argmax(logits, axis=1)\n",
        "    acc = accuracy_metric.compute(predictions=pred_ids, references=labels)\n",
        "    return {\"accuracy\": acc[\"accuracy\"]}\n",
        "\n",
        "# -------------------------\n",
        "# Main Logic\n",
        "# -------------------------\n",
        "def main():\n",
        "    torch.manual_seed(args.seed)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"--- Using device: {device.upper()} ---\")\n",
        "\n",
        "    if not os.path.exists(args.csv):\n",
        "        raise FileNotFoundError(f\"CSV file not found: {args.csv}\")\n",
        "\n",
        "    df = pd.read_csv(args.csv)\n",
        "\n",
        "    unique_winners = sorted(df[\"winner\"].unique().tolist())\n",
        "    label2id = {label: i for i, label in enumerate(unique_winners)}\n",
        "    id2label = {i: label for i, label in enumerate(unique_winners)}\n",
        "    num_labels = len(unique_winners)\n",
        "    print(f\"Found {num_labels} unique labels: {unique_winners}\")\n",
        "    print(f\"Label mapping: {label2id}\")\n",
        "    # Save the label mappings alongside the model adapter for inference\n",
        "    mappings_path = os.path.join(args.output_dir, \"label_mappings.json\")\n",
        "    with open(mappings_path, \"w\") as f:\n",
        "        json.dump({\"id2label\": id2label, \"label2id\": label2id}, f)\n",
        "    print(f\"Label mappings saved to {mappings_path}\")\n",
        "    # --------------------\n",
        "    df['label'] = df['winner'].map(label2id)\n",
        "\n",
        "    features = Features({\n",
        "        'question_id': Value('int64'),\n",
        "        'turn': Value('int64'),\n",
        "        'turn_1_query': Value('string'),\n",
        "        'turn_1_answer': Value('string'),\n",
        "        'turn_2_query': Value('string'),\n",
        "        'winner': Value('string'),\n",
        "        'label': ClassLabel(names=unique_winners)\n",
        "    })\n",
        "\n",
        "    raw_all = Dataset.from_pandas(df, features=features)\n",
        "\n",
        "    train_val_split = raw_all.train_test_split(\n",
        "        test_size=args.test_size,\n",
        "        seed=args.seed_data_split,\n",
        "        stratify_by_column=\"label\"\n",
        "    )\n",
        "    test_ds = train_val_split[\"test\"]\n",
        "    train_val_ds = train_val_split[\"train\"]\n",
        "\n",
        "    train_split = train_val_ds.train_test_split(\n",
        "        test_size=args.validation_size,\n",
        "        seed=args.seed_data_split,\n",
        "        stratify_by_column=\"label\"\n",
        "    )\n",
        "    train_ds = train_split[\"train\"]\n",
        "    val_ds = train_split[\"test\"]\n",
        "\n",
        "    print(f\"Dataset splits created: train={len(train_ds)}, validation={len(val_ds)}, test={len(test_ds)}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=True)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        args.model_name,\n",
        "        num_labels=num_labels,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id,\n",
        "    )\n",
        "\n",
        "    # For BERT, we usually target query/key/value/projection layers for LoRA\n",
        "    target_modules = [\"query\", \"key\", \"value\", \"dense\"]\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        r=args.lora_r,\n",
        "        lora_alpha=args.lora_alpha,\n",
        "        lora_dropout=args.lora_dropout,\n",
        "        target_modules=target_modules,\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    print(\"Wrapped model with LoRA. Trainable parameters:\")\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    tokenized_train = train_ds.map(lambda examples: preprocess_function(examples, tokenizer, args), batched=True)\n",
        "    tokenized_val = val_ds.map(lambda examples: preprocess_function(examples, tokenizer, args), batched=True)\n",
        "    tokenized_test = test_ds.map(lambda examples: preprocess_function(examples, tokenizer, args), batched=True)\n",
        "\n",
        "    columns_to_remove = ['question_id', 'turn', 'turn_1_query', 'turn_1_answer', 'turn_2_query', 'winner', 'label']\n",
        "    tokenized_train = tokenized_train.remove_columns([col for col in columns_to_remove if col in tokenized_train.column_names])\n",
        "    tokenized_val = tokenized_val.remove_columns([col for col in columns_to_remove if col in tokenized_val.column_names])\n",
        "    tokenized_test = tokenized_test.remove_columns([col for col in columns_to_remove if col in tokenized_test.column_names])\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=args.output_dir,\n",
        "        eval_strategy=\"steps\",\n",
        "        per_device_train_batch_size=args.batch_size,\n",
        "        per_device_eval_batch_size=args.batch_size,\n",
        "        num_train_epochs=args.epochs,\n",
        "        learning_rate=args.lr,\n",
        "        save_total_limit=args.save_total_limit,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        logging_steps=args.logging_steps,\n",
        "        eval_steps=args.eval_steps,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=args.eval_steps,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        greater_is_better=True,\n",
        "        seed=args.seed,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_val,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    print(\"--- Starting Training ---\")\n",
        "    trainer.train()\n",
        "    print(\"--- Training Finished ---\")\n",
        "\n",
        "    print(\"\\n--- Evaluating on the held-out Test Set ---\")\n",
        "    test_results = trainer.evaluate(eval_dataset=tokenized_test)\n",
        "    print(\"Test Set Metrics:\")\n",
        "    print(test_results)\n",
        "\n",
        "    print(\"\\nSaving final PEFT adapter to:\", args.output_dir)\n",
        "    model.save_pretrained(args.output_dir)\n",
        "    tokenizer.save_pretrained(args.output_dir)\n",
        "    print(\"Done.\")\n",
        "\n",
        "# Run the main function\n",
        "main()\n"
      ],
      "metadata": {
        "id": "G2tsECXUVkzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Saving to google drive functionality"
      ],
      "metadata": {
        "id": "6XccPqbdkoTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path in Google Drive where you want to save the model\n",
        "# You can change 'my_finetuned_model' to a different folder name if you prefer\n",
        "GOOGLE_DRIVE_SAVE_PATH = '/content/drive/MyDrive/ANLP Assignment 1'\n",
        "\n",
        "# Create the directory in Google Drive if it doesn't exist\n",
        "os.makedirs(GOOGLE_DRIVE_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "print(f\"Google Drive mounted at /content/drive\")\n",
        "print(f\"Model will be saved to {GOOGLE_DRIVE_SAVE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jtFkEU3j6Y4",
        "outputId": "6c438c7a-da9f-4a2a-c35d-cb33418d272f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted at /content/drive\n",
            "Model will be saved to /content/drive/MyDrive/ANLP Assignment 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory where the model adapter is saved locally after training\n",
        "LOCAL_MODEL_DIR = \"./model_adapter\"\n",
        "\n",
        "# Define the destination path in Google Drive\n",
        "GOOGLE_DRIVE_DEST_DIR = GOOGLE_DRIVE_SAVE_PATH\n",
        "\n",
        "# Copy the entire model adapter directory to Google Drive\n",
        "if os.path.exists(LOCAL_MODEL_DIR):\n",
        "    # Remove the destination directory in Google Drive if it already exists to avoid errors during copy\n",
        "    if os.path.exists(GOOGLE_DRIVE_DEST_DIR):\n",
        "        print(f\"Removing existing directory in Google Drive: {GOOGLE_DRIVE_DEST_DIR}\")\n",
        "        shutil.rmtree(GOOGLE_DRIVE_DEST_DIR)\n",
        "\n",
        "    print(f\"Copying model adapter from {LOCAL_MODEL_DIR} to {GOOGLE_DRIVE_DEST_DIR}\")\n",
        "    shutil.copytree(LOCAL_MODEL_DIR, GOOGLE_DRIVE_DEST_DIR)\n",
        "    print(\"Model adapter successfully saved to Google Drive.\")\n",
        "else:\n",
        "    print(f\"Local model directory not found: {LOCAL_MODEL_DIR}. Please run the training code first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X12mEcGXkV8Z",
        "outputId": "356583ac-fff7-41f4-f302-39d88a6148a6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing existing directory in Google Drive: /content/drive/MyDrive/ANLP Assignment 1\n",
            "Copying model adapter from ./model_adapter to /content/drive/MyDrive/ANLP Assignment 1\n",
            "Model adapter successfully saved to Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Inference (loads the fine-tuned model previously saved in google drive)"
      ],
      "metadata": {
        "id": "fZKnu196eTy2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e14c0353"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from peft import PeftModel, PeftConfig\n",
        "import json\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive if it's not already mounted\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Define the path in Google Drive where the model is saved\n",
        "GOOGLE_DRIVE_SAVED_MODEL_PATH = '/content/drive/MyDrive/ANLP Assignment 1' # Make sure this matches the save path\n",
        "\n",
        "def predict(query: str, model_dir: str):\n",
        "    \"\"\"\n",
        "    Loads a PEFT model and tokenizer from a directory and performs inference.\n",
        "\n",
        "    Args:\n",
        "        query (str): The input text (user query) to classify.\n",
        "        model_dir (str): The directory containing the PEFT adapter and artifacts (in Google Drive).\n",
        "\n",
        "    Returns:\n",
        "        str: The predicted class label (the best model name).\n",
        "    \"\"\"\n",
        "    # --- 1. Load All Artifacts from the Directory ---\n",
        "\n",
        "    # Load the PEFT config to get the base model name\n",
        "    config = PeftConfig.from_pretrained(model_dir)\n",
        "    base_model_name = config.base_model_name_or_path\n",
        "\n",
        "    # Load the label mappings\n",
        "    mappings_path = os.path.join(model_dir, \"label_mappings.json\")\n",
        "    with open(mappings_path, \"r\") as f:\n",
        "        label_mappings = json.load(f)\n",
        "        # The keys in the JSON file are strings, convert them back to integers\n",
        "        id2label = {int(k): v for k, v in label_mappings[\"id2label\"].items()}\n",
        "        label2id = label_mappings[\"label2id\"]\n",
        "\n",
        "    num_labels = len(id2label)\n",
        "\n",
        "    # Load the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "    # --- 2. Build the Model ---\n",
        "\n",
        "    # Load the base model with the correct classification head\n",
        "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        base_model_name,\n",
        "        num_labels=num_labels,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id,\n",
        "    )\n",
        "\n",
        "    # Apply the LoRA adapter\n",
        "    model = PeftModel.from_pretrained(base_model, model_dir)\n",
        "    model.eval()\n",
        "\n",
        "    # --- 3. Perform Inference ---\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    predicted_id = torch.argmax(logits, dim=-1).item()\n",
        "    predicted_label = model.config.id2label[predicted_id]\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Use the Google Drive path for inference\n",
        "    ADAPTER_DIRECTORY = GOOGLE_DRIVE_SAVED_MODEL_PATH\n",
        "\n",
        "    # Example query for inference\n",
        "    test_query = \"Query: Describe a vivid and unique character, using strong imagery and creative language. Please answer in fewer than two paragraphs.\\\n",
        "    Zephyr was a freckled-faced, wild-haired wanderer with eyes like the ocean, forever shifting from blue to green to grey. His voice was honeyed, smooth as silk and deep as a canyon, with a laugh that could shake the very foundations of the earth. His skin was tanned and weathered from a life spent under the open sky, and his hands were calloused and scarred from countless adventures.\\\n",
        "    He wore a patchwork coat made of scraps of leather and fur, adorned with feathers and beads that jangled with every step. His boots were sturdy and well-worn, and his hat was a wide-brimmed affair that shaded his face from the sun. He carried a staff made of gnarled wood, adorned with charms and trinkets that tinkled in the breeze. Zephyr was a solitary soul, always on the move, but those who crossed his path were forever changed by his contagious laughter and boundless spirit.\\\n",
        "    Revise your previous response and incorporate an allusion to a famous work of literature or historical event in each sentence.\"\n",
        "\n",
        "    # Get the prediction\n",
        "    best_model = predict(test_query, ADAPTER_DIRECTORY)\n",
        "\n",
        "    print(f\"Input Query:\\n'{test_query}'\")\n",
        "    print(\"---\")\n",
        "    print(f\"Predicted Best Model: {best_model}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEgtz0d6j2SW",
        "outputId": "8f6bd4f0-20d7-44fc-fdde-2e76ecdeb26b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Query:\n",
            "'Query: Describe a vivid and unique character, using strong imagery and creative language. Please answer in fewer than two paragraphs.    Zephyr was a freckled-faced, wild-haired wanderer with eyes like the ocean, forever shifting from blue to green to grey. His voice was honeyed, smooth as silk and deep as a canyon, with a laugh that could shake the very foundations of the earth. His skin was tanned and weathered from a life spent under the open sky, and his hands were calloused and scarred from countless adventures.    He wore a patchwork coat made of scraps of leather and fur, adorned with feathers and beads that jangled with every step. His boots were sturdy and well-worn, and his hat was a wide-brimmed affair that shaded his face from the sun. He carried a staff made of gnarled wood, adorned with charms and trinkets that tinkled in the breeze. Zephyr was a solitary soul, always on the move, but those who crossed his path were forever changed by his contagious laughter and boundless spirit.    Revise your previous response and incorporate an allusion to a famous work of literature or historical event in each sentence.'\n",
            "---\n",
            "Predicted Best Model: gpt-3.5-turbo\n"
          ]
        }
      ]
    }
  ]
}